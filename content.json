{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"https://HeHeHuang.github.io","root":"/"},"pages":[{"title":"关于","date":"2022-09-23T13:06:24.649Z","updated":"2022-09-23T13:02:55.707Z","comments":false,"path":"about/index.html","permalink":"https://hehehuang.github.io/about/index.html","excerpt":"","text":"个人详细介绍 title: 标签layout: tagscomments: false"},{"title":"分类","date":"2022-09-21T09:10:16.325Z","updated":"2022-09-21T05:03:10.208Z","comments":false,"path":"categories/index.html","permalink":"https://hehehuang.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-09-21T09:03:33.795Z","updated":"2022-09-21T05:03:10.208Z","comments":true,"path":"links/index.html","permalink":"https://hehehuang.github.io/links/index.html","excerpt":"","text":""},{"title":"书单","date":"2022-09-23T13:01:27.368Z","updated":"2022-09-21T05:03:10.207Z","comments":false,"path":"books/index.html","permalink":"https://hehehuang.github.io/books/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-09-24T09:44:30.595Z","updated":"2022-09-24T09:44:30.582Z","comments":false,"path":"tags/index.html","permalink":"https://hehehuang.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-09-21T09:10:00.214Z","updated":"2022-09-21T05:03:10.208Z","comments":false,"path":"repository/index.html","permalink":"https://hehehuang.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"NLP for DeepLearning","slug":"NLP-for-DeepLearning","date":"2022-09-24T10:29:33.000Z","updated":"2022-09-24T10:30:22.233Z","comments":true,"path":"2022/09/24/NLP-for-DeepLearning/","link":"","permalink":"https://hehehuang.github.io/2022/09/24/NLP-for-DeepLearning/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://hehehuang.github.io/categories/NLP/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://hehehuang.github.io/tags/Deep-Learning/"}]},{"title":"English Weekly Plan","slug":"English-Weekly-Plan","date":"2022-09-24T09:45:12.000Z","updated":"2022-09-24T10:01:15.068Z","comments":true,"path":"2022/09/24/English-Weekly-Plan/","link":"","permalink":"https://hehehuang.github.io/2022/09/24/English-Weekly-Plan/","excerpt":"","text":"THINGS YOU WILL NEED: 1 hour per day (30 minutes will work as well). a notebook and a pen to write down new vocabulary each day. an English song. an English a book. an English TV show. MONDAYS (Spend 30-40 minutes Reading) Read your book. Read the news. Read an article on http://simple.wikipedia.com Write down some new words in your notebook with their definition. Spend the rest of your hour: – Reading out loud from your book – Writing out some passages from your book. – Listening to the audiobook of your book. – Using Duolingo or other learning apps. TUESDAYS (Spend 30-40 minutes Writing) Write what you did yesterday, today, and tomorrow in your notebook. Write a few comments on Youtube videos. Write some sentences about what you like and dislike about your song, your book, and your TV show. Write a summary of what you have read in your book the day before. Put some new words in your dictionary. Spend the rest of your hour: – Reading what you have written. – Listening to what you have written by pasting your writing into Google translate and have it read it to you. – Reading what you have written out loud. – Using Duolingo or other learning apps. WEDNESDAYS (Spend 30-40 minutes Listening): Watch your TV show. Listen to your song. Listen to the Audiobook of your book if you have it. Watch a few Youtube videos. Put new words in your dictionary. Spend the rest of your hour: – Read about your TV show. – Write about how your song makes you feel. – Mimic or shadow people from your TV show. – Sing along to your song. THURSDAYS (Spend 30-40 minutes Speaking): Hire a tutor on: – Preply: http://tracking.preply.com/SH2X – Cambly: http://cambly.com – iTalki: http://italki.com Talk out loud. Narrate your life. Record yourself and play it back. Put words in your dictionary. Spend the rest of your hour: – Reading out loud. – Singing out loud. – Shadowing or mimicking actors from your TV show. FRIDAYS (Spend the Entire hour on Vocabulary): Review all the words in your dictionary. Write them out. Say them out loud. Use them in a sentence. Make a http://quizlet.com set. Make flashcards. Work until you have them all memorized. WEEKENDS: Find a new song. Find a new book. Find a new TV show. Take a break. References:https://www.youtube.com/watch?v=5-T6Xqlh6BU&amp;t=283s","categories":[{"name":"English","slug":"English","permalink":"https://hehehuang.github.io/categories/English/"}],"tags":[]},{"title":"Write an article","slug":"Write-an-artical","date":"2022-09-24T08:56:07.000Z","updated":"2022-09-24T09:43:02.358Z","comments":true,"path":"2022/09/24/Write-an-artical/","link":"","permalink":"https://hehehuang.github.io/2022/09/24/Write-an-artical/","excerpt":"","text":"Five step to write an article1. Ask a question2. Write a thesis statement (answer the question)3. Write an introductionHow to write an introduction Grabbers (attention) Joke Proverb&#x2F;quote anecdote surprising fact curiosity(?) Topic(subject) -what reasons(or main ideas) (3) Thesis -argument whether the topic is negative and positive 5w1h 4. Prove your thesisEssay Body (Structure) How to write an paragraph Topic sentence &amp; Closing sentence identifies the topic states what will be said about the topic Specific details &amp; supporting facts Specific details &amp; supporting faccts （for example: Facts, example from a source) Your thinking about the facts your own thoughts about the details &amp; facts (analogy, analysis, interpretation or explanation of facts) 5. Write your conclusion6. Extra Topichow to write a summarizehow to write a comment Reference:https://www.youtube.com/watch?v=Mwv3OlYojWw&amp;t=505s https://www.youtube.com/watch?v=IN6IOSMviS4 https://www.youtube.com/watch?v=jLdvEFtUuMM&amp;t=56s","categories":[{"name":"English","slug":"English","permalink":"https://hehehuang.github.io/categories/English/"}],"tags":[{"name":"Writing","slug":"Writing","permalink":"https://hehehuang.github.io/tags/Writing/"}]},{"title":"Machine Learning","slug":"Machine-Learning","date":"2022-09-23T13:04:19.000Z","updated":"2022-09-23T13:04:19.039Z","comments":true,"path":"2022/09/23/Machine-Learning/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Machine-Learning/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Data Analysis with Pyspark","slug":"Data-Analysis-with-Pyspark","date":"2022-09-23T10:25:22.501Z","updated":"2022-09-23T10:36:13.511Z","comments":true,"path":"2022/09/23/Data-Analysis-with-Pyspark/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Data-Analysis-with-Pyspark/","excerpt":"","text":"","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://hehehuang.github.io/categories/Data-Analysis/"}],"tags":[{"name":"PySpark","slug":"PySpark","permalink":"https://hehehuang.github.io/tags/PySpark/"}]},{"title":"Data Analysis with SQL","slug":"Data-Analysis-with-SQL","date":"2022-09-23T10:24:38.833Z","updated":"2022-09-23T10:36:39.635Z","comments":true,"path":"2022/09/23/Data-Analysis-with-SQL/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Data-Analysis-with-SQL/","excerpt":"","text":"","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://hehehuang.github.io/categories/Data-Analysis/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hehehuang.github.io/tags/SQL/"}]},{"title":"Data Analysis with Python","slug":"Data-Analysis-with-Python","date":"2022-09-23T10:24:12.280Z","updated":"2022-09-23T10:37:04.870Z","comments":true,"path":"2022/09/23/Data-Analysis-with-Python/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Data-Analysis-with-Python/","excerpt":"","text":"","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://hehehuang.github.io/categories/Data-Analysis/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/tags/Python/"}]},{"title":"Python OOP","slug":"Python OOP","date":"2022-09-23T10:22:54.991Z","updated":"2022-09-23T10:23:04.903Z","comments":true,"path":"2022/09/23/Python OOP/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Python%20OOP/","excerpt":"","text":"","categories":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"}],"tags":[]},{"title":"Python Basic","slug":"Python Basic","date":"2022-09-23T10:21:30.629Z","updated":"2022-09-23T12:59:04.880Z","comments":true,"path":"2022/09/23/Python Basic/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Python%20Basic/","excerpt":"","text":"build in data type -","categories":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"}],"tags":[]},{"title":"Array & Linked List","slug":"Array & Linked List","date":"2022-09-23T10:20:09.606Z","updated":"2022-09-23T10:20:32.597Z","comments":true,"path":"2022/09/23/Array & Linked List/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Array%20&%20Linked%20List/","excerpt":"","text":"","categories":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://hehehuang.github.io/tags/Data-Structure/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://hehehuang.github.io/tags/Algorithm/"}]},{"title":"Data Engineer","slug":"Data Engineer","date":"2022-09-23T10:12:25.135Z","updated":"2022-09-23T10:16:35.833Z","comments":true,"path":"2022/09/23/Data Engineer/","link":"","permalink":"https://hehehuang.github.io/2022/09/23/Data%20Engineer/","excerpt":"","text":"Data Engineer What is Data Engineer? What is main duly for data engineer? how data engineer work with AWS? what can i learn for data engineer? Big Data: data pipeline, Cloud service","categories":[{"name":"Big Data","slug":"Big-Data","permalink":"https://hehehuang.github.io/categories/Big-Data/"},{"name":"Cloud Service","slug":"Big-Data/Cloud-Service","permalink":"https://hehehuang.github.io/categories/Big-Data/Cloud-Service/"}],"tags":[{"name":"Data Warehouse","slug":"Data-Warehouse","permalink":"https://hehehuang.github.io/tags/Data-Warehouse/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://hehehuang.github.io/tags/NoSQL/"},{"name":"Data Pipeline","slug":"Data-Pipeline","permalink":"https://hehehuang.github.io/tags/Data-Pipeline/"},{"name":"AWS","slug":"AWS","permalink":"https://hehehuang.github.io/tags/AWS/"}]},{"title":"Tree","slug":"Tree","date":"2022-09-22T08:36:43.586Z","updated":"2022-09-22T08:42:39.475Z","comments":true,"path":"2022/09/22/Tree/","link":"","permalink":"https://hehehuang.github.io/2022/09/22/Tree/","excerpt":"","text":"Tree is the basic data Structure 12def searchTree():","categories":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://hehehuang.github.io/tags/Data-Structure/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://hehehuang.github.io/tags/Algorithm/"}]},{"title":"Sorting","slug":"Sorting","date":"2022-09-22T08:34:21.026Z","updated":"2022-09-22T08:42:36.580Z","comments":true,"path":"2022/09/22/Sorting/","link":"","permalink":"https://hehehuang.github.io/2022/09/22/Sorting/","excerpt":"","text":"sorting is algorithm to sort list in order. There are ten basic algorithm:","categories":[{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://hehehuang.github.io/tags/Algorithm/"}]},{"title":"AWS","slug":"AWS","date":"2022-09-19T17:45:57.203Z","updated":"2022-09-24T10:01:40.740Z","comments":true,"path":"2022/09/20/AWS/","link":"","permalink":"https://hehehuang.github.io/2022/09/20/AWS/","excerpt":"","text":"What is AWS?Big data frameworkwhat is componence of AWS with Big Data ecosystem","categories":[],"tags":[]},{"title":"Hadoop","slug":"Hadoop","date":"2022-09-19T17:45:57.203Z","updated":"2022-09-24T10:01:40.566Z","comments":true,"path":"2022/09/20/Hadoop/","link":"","permalink":"https://hehehuang.github.io/2022/09/20/Hadoop/","excerpt":"","text":"What is HadoopHadoop &#x3D; HDFS + MapReduce Why need Hadoop?because the cpu, distrbution computing. what is changes History of HadoopHadoop 1.0Hadoop 2.0Hadoop 3.0HDFSMapReducehow data is processed in MapReduce Map stage + Reduce shuffling in Partition the number of partitioner is decide by the number of Reduce task","categories":[{"name":"Big Data","slug":"Big-Data","permalink":"https://hehehuang.github.io/categories/Big-Data/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hehehuang.github.io/tags/Hadoop/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://hehehuang.github.io/tags/Mapreduce/"}]},{"title":"PySpark","slug":"PySpark","date":"2022-09-19T17:45:57.203Z","updated":"2022-09-22T12:46:25.000Z","comments":false,"path":"2022/09/20/PySpark/","link":"","permalink":"https://hehehuang.github.io/2022/09/20/PySpark/","excerpt":"","text":"What is Spark?Spark is a data processing engine for big data sets. Pyspark is that Spark can be call by python language. why need Spark?Advantages of spark handle many case: SparkSQL(dataframe) for data analysis Spark streaming for real-time analysis Spark Ml for machine learning work with many source Standalone, Hadoop, K8s,AWS better than Mapreduce, enhancement to mapreduce (in real-time processing) Why Spark is better than Mapreduce in terms of efficiency Mapreduce takes input from the HDFS, each map task process it , keeps in memory ,if exceed the default 100MB(io.sort.mb), then it split to disk.reduce task take data from disk and continue to process it. Spark uses random access memory (RAM) to cache and process data instead of a file system. Spark creates a Directed Acyclic Graph (DAG) to schedule tasks and the orchestration of nodes across the Hadoop cluster. How Spark work in cluster?HDFS,Yarn,Spark HDFS: Name node, Data Node Yarn: Resource Manager, Node Manager Spark: Driver, Worker when spark start to work, it need HDFS to provide the data and Yarn to provide computing resource. Spark RDDFeatures: partitions distributed in cluster computing each split dependencies on each RDDs (lineage) Key-value RDDs have Partioner sent computing to data Two step in RDD: Transform: transform one RDD to another RDD 123456789from pyspark import SparkContextfrom pyspark import SparkConfconf = SparkConf()sc = SparkContext(conf=conf)rdd1 = sc.parallelize([1,2,3,4])rdd2 = rdd1.map(lambda x: x+1) Action: take action for the RDD, normally, the output will be value 1result = rdd2.collect() RDD Persistence and Caching Mechanismif there are two actions in an spark application. 123456rdd1 = sc.textfile(//:HDFS..)rdd2 = sc.map(lambda x: x+1)# one action rdd3 = rdd2.count()# another actionrdd4 = rdd2.count() one action means one job in spark application. the rdd will be released in memory, which is that rdd3 will be cleaned. the rdd4 is the result of action at the rdd2. it will be causing that the re-calculation of rdd2. for this senaria, we need the RDD Persistence and cache to save our time and cost. serveral caching mechanism is available. Spark narrow and wide transformationNarrow transformation provides one to one transformation which there is no shuffling of data cross the nodes (every node do their own work can the results are concatenated). Some examples of narrow transformation include: map(), flatMap(), filter(), sample(), union(). For wide transformation, all the elements are required to do the operation. Partition live in many partitions of parent RDD which results in shuffling of data across the nodes. Some examples of wide transformation include: intersection(), distinct(), groupByKey(), reduceByKey(). Spark CoreSpark SqlSpark StreamingSpark MLReferencehttps://www.ibm.com/cloud/blog/hadoop-vs-spark https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-streaming-high-availability https://cedricyf.medium.com/get-started-on-apache-pyspark-part-2-15d62494c18a","categories":[{"name":"Big Data","slug":"Big-Data","permalink":"https://hehehuang.github.io/categories/Big-Data/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hehehuang.github.io/tags/Hadoop/"}]}],"categories":[{"name":"NLP","slug":"NLP","permalink":"https://hehehuang.github.io/categories/NLP/"},{"name":"English","slug":"English","permalink":"https://hehehuang.github.io/categories/English/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://hehehuang.github.io/categories/Data-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/categories/Python/"},{"name":"Big Data","slug":"Big-Data","permalink":"https://hehehuang.github.io/categories/Big-Data/"},{"name":"Cloud Service","slug":"Big-Data/Cloud-Service","permalink":"https://hehehuang.github.io/categories/Big-Data/Cloud-Service/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://hehehuang.github.io/tags/Deep-Learning/"},{"name":"Writing","slug":"Writing","permalink":"https://hehehuang.github.io/tags/Writing/"},{"name":"PySpark","slug":"PySpark","permalink":"https://hehehuang.github.io/tags/PySpark/"},{"name":"SQL","slug":"SQL","permalink":"https://hehehuang.github.io/tags/SQL/"},{"name":"Python","slug":"Python","permalink":"https://hehehuang.github.io/tags/Python/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"https://hehehuang.github.io/tags/Data-Structure/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://hehehuang.github.io/tags/Algorithm/"},{"name":"Data Warehouse","slug":"Data-Warehouse","permalink":"https://hehehuang.github.io/tags/Data-Warehouse/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://hehehuang.github.io/tags/NoSQL/"},{"name":"Data Pipeline","slug":"Data-Pipeline","permalink":"https://hehehuang.github.io/tags/Data-Pipeline/"},{"name":"AWS","slug":"AWS","permalink":"https://hehehuang.github.io/tags/AWS/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://hehehuang.github.io/tags/Hadoop/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://hehehuang.github.io/tags/Mapreduce/"}]}